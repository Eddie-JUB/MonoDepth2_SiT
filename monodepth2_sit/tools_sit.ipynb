{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Data pre-prossesing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1 clean up unnesessary files and convert to kitti dataset format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sit to kitti convert\n",
    "\n",
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def cleanup_dataset(root_dir):\n",
    "    folders_to_delete = ['bev', 'ego_trajectory', 'imu', 'label_2d', 'label_3d', 'rtk', 'tf']\n",
    "\n",
    "    print(\"Cleaning up dataset...\")\n",
    "    for folder in folders_to_delete:\n",
    "        for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "            if folder in dirnames:\n",
    "                folder_path = os.path.join(dirpath, folder)\n",
    "                shutil.rmtree(folder_path, ignore_errors=True)\n",
    "                # print(f\"Deleted folder: {folder_path}\")\n",
    "\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if 'cam_img' in dirnames:\n",
    "            cam_img_path = os.path.join(dirpath, 'cam_img')\n",
    "            for subdir in os.listdir(cam_img_path):\n",
    "                if subdir != '4':\n",
    "                    subdir_path = os.path.join(cam_img_path, subdir)\n",
    "                    if os.path.isdir(subdir_path):\n",
    "                        shutil.rmtree(subdir_path, ignore_errors=True)\n",
    "                        # print(f\"Deleted folder: {subdir_path}\")\n",
    "\n",
    "            if '4' in os.listdir(cam_img_path):\n",
    "                data_rgb_path = os.path.join(cam_img_path, '4', 'data_rgb')\n",
    "                if os.path.isdir(data_rgb_path):\n",
    "                    shutil.rmtree(data_rgb_path, ignore_errors=True)\n",
    "                    # print(f\"Deleted folder: {data_rgb_path}\")\n",
    "\n",
    "                data_undist_path = os.path.join(cam_img_path, '4', 'data_undist')\n",
    "                if os.path.isdir(data_undist_path):\n",
    "                    new_path = os.path.join(dirpath, 'image_02')\n",
    "                    os.rename(data_undist_path, new_path)\n",
    "                    # print(f\"Renamed folder: {data_undist_path} to {new_path}\")\n",
    "\n",
    "        if 'velo' in dirnames:\n",
    "            velo_path = os.path.join(dirpath, 'velo')\n",
    "            for subdir in os.listdir(velo_path):\n",
    "                if subdir != 'concat':\n",
    "                    subdir_path = os.path.join(velo_path, subdir)\n",
    "                    if os.path.isdir(subdir_path):\n",
    "                        shutil.rmtree(subdir_path, ignore_errors=True)\n",
    "                        # print(f\"Deleted folder: {subdir_path}\")\n",
    "\n",
    "    # After processing, remove cam_img folder if it exists\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if 'cam_img' in dirnames:\n",
    "            cam_img_path = os.path.join(dirpath, 'cam_img')\n",
    "            shutil.rmtree(cam_img_path, ignore_errors=True)\n",
    "            # print(f\"Deleted folder: {cam_img_path}\")\n",
    "\n",
    "    # Remove data folder inside velo/concat folder\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if 'velo' in dirnames:\n",
    "            velo_path = os.path.join(dirpath, 'velo')\n",
    "            concat_path = os.path.join(velo_path, 'concat')\n",
    "            data_path = os.path.join(concat_path, 'data')\n",
    "            if os.path.isdir(data_path):\n",
    "                shutil.rmtree(data_path, ignore_errors=True)\n",
    "                # print(f\"Deleted folder: {data_path}\")\n",
    "\n",
    "    # Rename bin_data folder to data inside velo/concat\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if 'velo' in dirnames:\n",
    "            velo_path = os.path.join(dirpath, 'velo')\n",
    "            concat_path = os.path.join(velo_path, 'concat')\n",
    "            bin_data_path = os.path.join(concat_path, 'bin_data')\n",
    "            if os.path.isdir(bin_data_path):\n",
    "                new_data_path = os.path.join(concat_path, 'data')\n",
    "                os.rename(bin_data_path, new_data_path)\n",
    "                # print(f\"Renamed folder: {bin_data_path} to {new_data_path}\")\n",
    "\n",
    "def move_image_files(root_dir):\n",
    "    print(\"Moving image files...\")\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if 'image_02' in dirnames:\n",
    "            image_02_path = os.path.join(dirpath, 'image_02')\n",
    "            data_path = os.path.join(image_02_path, 'data')\n",
    "            os.makedirs(data_path, exist_ok=True)\n",
    "\n",
    "            files_to_move = [f for f in os.listdir(image_02_path) if os.path.isfile(os.path.join(image_02_path, f))]\n",
    "            for file in tqdm(files_to_move, desc=f\"Moving files in {image_02_path}\", unit=\"files\"):\n",
    "                shutil.move(os.path.join(image_02_path, file), os.path.join(data_path, file))\n",
    "\n",
    "\n",
    "def rename_velo_folder(root_dir):\n",
    "    print(\"Renaming velo folders...\")\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if 'velo' in dirnames:\n",
    "            velo_path = os.path.join(dirpath, 'velo')\n",
    "            new_velo_path = os.path.join(dirpath, 'velodyne_points')\n",
    "            os.rename(velo_path, new_velo_path)\n",
    "            # print(f\"Renamed folder: {velo_path} to {new_velo_path}\")\n",
    "\n",
    "\n",
    "def rename_files(root_dir):\n",
    "    print(\"Renaming files...\")\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        for filename in filenames:\n",
    "            if filename.endswith('.bin') or filename.endswith('.png'):\n",
    "                base, ext = os.path.splitext(filename)\n",
    "                try:\n",
    "                    new_base = f\"{int(base):010d}\"\n",
    "                except ValueError:\n",
    "                    continue\n",
    "                new_name = f\"{new_base}{ext}\"\n",
    "                old_file = os.path.join(dirpath, filename)\n",
    "                new_file = os.path.join(dirpath, new_name)\n",
    "                \n",
    "                # Check if new_file already exists\n",
    "                if os.path.exists(new_file):\n",
    "                    print(f\"File {new_file} already exists. Skipping rename for {old_file}\")\n",
    "                    continue\n",
    "                \n",
    "                os.rename(old_file, new_file)\n",
    "                # print(f\"Renamed file: {old_file} to {new_file}\")\n",
    "\n",
    "def move_concat_data_folder(root_dir):\n",
    "    print(\"Moving concat data folder and deleting concat folder...\")\n",
    "    for dirpath, dirnames, filenames in os.walk(root_dir):\n",
    "        if 'velodyne_points' in dirnames:\n",
    "            velodyne_points_path = os.path.join(dirpath, 'velodyne_points')\n",
    "            concat_path = os.path.join(velodyne_points_path, 'concat')\n",
    "            data_path = os.path.join(concat_path, 'data')\n",
    "            if os.path.isdir(data_path):\n",
    "                # Move data folder to velodyne_points_path\n",
    "                new_data_path = os.path.join(velodyne_points_path, 'data')\n",
    "                shutil.move(data_path, new_data_path)\n",
    "                # Delete concat folder\n",
    "                shutil.rmtree(concat_path, ignore_errors=True)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    root_dir = \"/media/eddie/Samsung_T5/test\"\n",
    "    cleanup_dataset(root_dir)\n",
    "    move_image_files(root_dir)\n",
    "    rename_velo_folder(root_dir)\n",
    "    move_concat_data_folder(root_dir)\n",
    "    rename_files(root_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.2 convert sit dataset split data format to kitti dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train.txt -> train_files_sit.txt\n",
    "\n",
    "def convert_to_train_files_format(input_file, output_file):\n",
    "    with open(input_file, 'r') as infile, open(output_file, 'w') as outfile:\n",
    "        lines = infile.readlines()\n",
    "        for line in lines:\n",
    "            components = line.strip().split('*')\n",
    "            if len(components) == 3:\n",
    "                scene, drive, frame = components\n",
    "                new_line = f\"{scene}/{drive} {frame} l\\n\"\n",
    "                outfile.write(new_line)\n",
    "\n",
    "input_files = [\n",
    "    '/media/eddie/28901C58901C2F36/dataset/SiT_Dataset_full_kitti/train.txt',\n",
    "    '/media/eddie/28901C58901C2F36/dataset/SiT_Dataset_full_kitti/val.txt',\n",
    "    '/media/eddie/28901C58901C2F36/dataset/SiT_Dataset_full_kitti/test.txt'\n",
    "]\n",
    "\n",
    "output_files = [\n",
    "    '/media/eddie/28901C58901C2F36/dataset/SiT_Dataset_full_kitti/train_files_sit.txt',\n",
    "    '/media/eddie/28901C58901C2F36/dataset/SiT_Dataset_full_kitti/val_files_sit.txt',\n",
    "    '/media/eddie/28901C58901C2F36/dataset/SiT_Dataset_full_kitti/test_files_sit.txt'\n",
    "]\n",
    "\n",
    "convert_to_train_files_format(input_files, output_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Detph Map GT generation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Depth map GT generated directly projection from lidar point cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Front point cloud projected to cam4(front, P3)\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import concurrent.futures\n",
    "\n",
    "def load_calibration(calib_file):\n",
    "    calib_data = {}\n",
    "    with open(calib_file) as f:\n",
    "        for line in f:\n",
    "            if line.startswith(\"P3_intrinsic:\"):\n",
    "                calib_data['P3_intrinsic'] = np.array([float(x) for x in line.split()[1:]]).reshape(3, 3)\n",
    "            elif line.startswith(\"P3_extrinsic:\"):\n",
    "                calib_data['P3_extrinsic'] = np.array([float(x) for x in line.split()[1:]]).reshape(3, 4)\n",
    "    return calib_data\n",
    "\n",
    "def load_point_cloud(bin_file):\n",
    "    points = np.fromfile(bin_file, dtype=np.float32).reshape(-1, 4)\n",
    "    return points[:, :3]\n",
    "\n",
    "def transform_point_cloud(points, R, T):\n",
    "    points_hom = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "    transformation_matrix = np.hstack((R, T))\n",
    "    transformation_matrix = np.vstack((transformation_matrix, [0, 0, 0, 1]))\n",
    "    points_transformed = points_hom.dot(transformation_matrix.T)\n",
    "    return points_transformed[:, :3]\n",
    "\n",
    "# filter out backward points\n",
    "def filter_forward_points(points):\n",
    "    return points[points[:, 2] > 0]\n",
    "\n",
    "def project_to_image(points, P):\n",
    "    points_hom = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "    points_2d = points_hom.dot(P.T)\n",
    "    points_2d[:, :2] /= points_2d[:, 2][:, np.newaxis]\n",
    "    return points_2d[:, :2], points_2d[:, 2]\n",
    "\n",
    "def create_depth_map(points_2d, depth, image_size):\n",
    "    depth_map = np.zeros(image_size)\n",
    "    for i, (x, y) in enumerate(points_2d):\n",
    "        if 0 <= x < image_size[1] and 0 <= y < image_size[0]:\n",
    "            if depth_map[int(y), int(x)] == 0 or depth_map[int(y), int(x)] > depth[i]:\n",
    "                depth_map[int(y), int(x)] = depth[i]\n",
    "    return depth_map\n",
    "\n",
    "def refine_depth_map(depth_map, inpaint_radius=3):\n",
    "    depth_map[depth_map == 0] = np.nan\n",
    "    refined_map = cv2.inpaint(depth_map.astype(np.float32), np.isnan(depth_map).astype(np.uint8), inpaint_radius, cv2.INPAINT_NS)\n",
    "    return refined_map\n",
    "\n",
    "def process_image(calib, img_file, lidar_file, output_dir, inpaint_radius):\n",
    "    P3_intrinsic = calib['P3_intrinsic']\n",
    "    P3_extrinsic = calib['P3_extrinsic']\n",
    "    \n",
    "    R = P3_extrinsic[:, :3]\n",
    "    T = P3_extrinsic[:, 3:4]\n",
    "    P = np.hstack((P3_intrinsic, np.zeros((3, 1))))\n",
    "    \n",
    "    points = load_point_cloud(lidar_file)\n",
    "    points = filter_forward_points(points)\n",
    "    points_cam = transform_point_cloud(points, R, T)\n",
    "    points_2d, depth = project_to_image(points_cam, P)\n",
    "    \n",
    "    image = cv2.imread(img_file)\n",
    "    image_size = image.shape[:2]\n",
    "    \n",
    "    depth_map = create_depth_map(points_2d, depth, image_size)\n",
    "    \n",
    "    # Refinement with adjustable inpaint radius\n",
    "    depth_map_refined = refine_depth_map(depth_map, inpaint_radius=inpaint_radius)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, os.path.basename(img_file))\n",
    "    cv2.imwrite(output_path, depth_map_refined)\n",
    "    # print(f\"Saved refined depth map for {img_file} at {output_path}\")\n",
    "\n",
    "def process_folder(base_dir, inpaint_radius=3):\n",
    "    calib_file = os.path.join(base_dir, 'calib', '0.txt')\n",
    "    image_dir = os.path.join(base_dir, 'image_02', 'data')\n",
    "    lidar_dir = os.path.join(base_dir, 'velodyne_points', 'data')\n",
    "    output_dir = os.path.join(base_dir, 'proj_depth', 'groundtruth', 'image_02')\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    calib = load_calibration(calib_file)\n",
    "    \n",
    "    image_files = sorted(os.listdir(image_dir))\n",
    "    lidar_files = sorted(os.listdir(lidar_dir))\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for img_file, lidar_file in zip(image_files, lidar_files):\n",
    "            img_path = os.path.join(image_dir, img_file)\n",
    "            lidar_path = os.path.join(lidar_dir, lidar_file)\n",
    "            futures.append(executor.submit(process_image, calib, img_path, lidar_path, output_dir, inpaint_radius))\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "def main(root_dir, inpaint_radius=3):\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        if 'calib' in dirs and 'image_02' in dirs and 'velodyne_points' in dirs:\n",
    "            print(f\"Processing folder: {subdir}\")\n",
    "            process_folder(subdir, inpaint_radius=inpaint_radius)\n",
    "\n",
    "root_dir = '/home/eddie/depth_estimation/monodepth2/sit_test_r3'\n",
    "inpaint_radius = 2  # Adjust this value to refine the depth map less or more\n",
    "main(root_dir, inpaint_radius=inpaint_radius)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Depth map GT using interpolation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import concurrent.futures\n",
    "from scipy.interpolate import griddata\n",
    "\n",
    "def load_calibration(calib_file):\n",
    "    calib_data = {}\n",
    "    with open(calib_file) as f:\n",
    "        for line in f:\n",
    "            key, value = line.split(':', 1)\n",
    "            calib_data[key] = np.array([float(x) for x in value.split()])\n",
    "    return calib_data\n",
    "\n",
    "def load_point_cloud(bin_file):\n",
    "    points = np.fromfile(bin_file, dtype=np.float32).reshape(-1, 4)\n",
    "    return points[:, :3]\n",
    "\n",
    "def transform_and_filter_points(points, calib):\n",
    "    P3 = calib['P3_intrinsic'].reshape(3, 3)\n",
    "    R0_rect = calib['R0_rect'].reshape(3, 3)\n",
    "    P3_extrinsic = calib['P3_extrinsic'].reshape(3, 4)\n",
    "    \n",
    "    points = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "    points = np.dot(points, P3_extrinsic.T)\n",
    "    points = np.dot(points, R0_rect.T)\n",
    "    \n",
    "    points = points[points[:, 2] > 0]\n",
    "    \n",
    "    points = np.dot(points, P3.T)\n",
    "    \n",
    "    points[:, 0] /= points[:, 2]\n",
    "    points[:, 1] /= points[:, 2]\n",
    "    \n",
    "    return points[:, :2], points[:, 2]\n",
    "\n",
    "def create_depth_map(points_2d, depth, image_size, point_size=1):\n",
    "    depth_map = np.full(image_size, np.nan)\n",
    "    for i, (x, y) in enumerate(points_2d):\n",
    "        if 0 <= x < image_size[1] and 0 <= y < image_size[0]:\n",
    "            for dx in range(-point_size, point_size + 1):\n",
    "                for dy in range(-point_size, point_size + 1):\n",
    "                    if 0 <= x + dx < image_size[1] and 0 <= y + dy < image_size[0]:\n",
    "                        if np.isnan(depth_map[int(y + dy), int(x + dx)]) or depth_map[int(y + dy), int(x + dx)] > depth[i]:\n",
    "                            depth_map[int(y + dy), int(x + dx)] = depth[i]\n",
    "    return depth_map\n",
    "\n",
    "def interpolate_depth_map(depth_map, method='linear'):\n",
    "    x = np.arange(depth_map.shape[1])\n",
    "    y = np.arange(depth_map.shape[0])\n",
    "    xx, yy = np.meshgrid(x, y)\n",
    "    known_points = np.array(np.where(~np.isnan(depth_map))).T\n",
    "    known_values = depth_map[~np.isnan(depth_map)]\n",
    "    interpolated_map = griddata(known_points, known_values, (yy, xx), method=method, fill_value=0)\n",
    "    return interpolated_map\n",
    "\n",
    "def process_image(calib, img_file, lidar_file, output_dir, interpolation_method, point_size):\n",
    "    points = load_point_cloud(lidar_file)\n",
    "    points_2d, depth = transform_and_filter_points(points, calib)\n",
    "    \n",
    "    image = cv2.imread(img_file)\n",
    "    image_size = image.shape[:2]\n",
    "    \n",
    "    depth_map = create_depth_map(points_2d, depth, image_size, point_size=point_size)\n",
    "    \n",
    "    # Interpolation with adjustable method\n",
    "    depth_map_interpolated = interpolate_depth_map(depth_map, method=interpolation_method)\n",
    "    \n",
    "    output_path = os.path.join(output_dir, os.path.basename(img_file))\n",
    "    cv2.imwrite(output_path, (depth_map_interpolated * 256).astype(np.uint16))\n",
    "    # print(f\"Saved interpolated depth map for {img_file} at {output_path}\")\n",
    "\n",
    "def process_folder(base_dir, interpolation_method='linear', point_size=1):\n",
    "    calib_file = os.path.join(base_dir, 'calib', '0.txt')\n",
    "    image_dir = os.path.join(base_dir, 'image_02', 'data')\n",
    "    lidar_dir = os.path.join(base_dir, 'velodyne_points', 'data')\n",
    "    output_dir = os.path.join(base_dir, 'proj_depth', 'groundtruth', 'image_02')\n",
    "    \n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    calib = load_calibration(calib_file)\n",
    "    \n",
    "    image_files = sorted(os.listdir(image_dir))\n",
    "    lidar_files = sorted(os.listdir(lidar_dir))\n",
    "    \n",
    "    with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "        futures = []\n",
    "        for img_file, lidar_file in zip(image_files, lidar_files):\n",
    "            img_path = os.path.join(image_dir, img_file)\n",
    "            lidar_path = os.path.join(lidar_dir, lidar_file)\n",
    "            futures.append(executor.submit(process_image, calib, img_path, lidar_path, output_dir, interpolation_method, point_size))\n",
    "        for future in concurrent.futures.as_completed(futures):\n",
    "            future.result()\n",
    "\n",
    "def main(root_dir, interpolation_method='linear', point_size=1):\n",
    "    for subdir, dirs, files in os.walk(root_dir):\n",
    "        if 'calib' in dirs and 'image_02' in dirs and 'velodyne_points' in dirs:\n",
    "            print(f\"Processing folder: {subdir}\")\n",
    "            process_folder(subdir, interpolation_method=interpolation_method, point_size=point_size)\n",
    "\n",
    "root_dir = '/home/eddie/depth_estimation/monodepth2/sit_test_r3'\n",
    "interpolation_method = 'linear'  # Adjust this value to 'linear', 'nearest', or 'cubic'\n",
    "point_size = 2  # Adjust this value to set the size of the points\n",
    "main(root_dir, interpolation_method=interpolation_method, point_size=point_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.1 Project point cloud to img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# points projection to img\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "def load_velodyne_points(file_path):\n",
    "    points = np.fromfile(file_path, dtype=np.float32).reshape(-1, 4)\n",
    "    return points\n",
    "\n",
    "def load_calibration(file_path):\n",
    "    calib = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            key, value = line.split(':', 1)\n",
    "            calib[key] = np.array([float(x) for x in value.split()])\n",
    "    return calib\n",
    "\n",
    "\n",
    "def project_velo_to_cam2(points, calib):\n",
    "    P3 = calib['P3_intrinsic'].reshape(3, 3)\n",
    "    R0_rect = calib['R0_rect'].reshape(3, 3)\n",
    "    P3_extrinsic = calib['P3_extrinsic'].reshape(3, 4)\n",
    "    \n",
    "    points = points[:, :3]\n",
    "    points = np.hstack((points, np.ones((points.shape[0], 1))))\n",
    "    points = np.dot(points, P3_extrinsic.T)\n",
    "    points = np.dot(points, R0_rect.T)\n",
    "    \n",
    "    points = points[points[:, 2] > 0]\n",
    "    \n",
    "    points = np.dot(points, P3.T)\n",
    "    \n",
    "    points[:, 0] /= points[:, 2]\n",
    "    points[:, 1] /= points[:, 2]\n",
    "    \n",
    "    return points\n",
    "\n",
    "\n",
    "\n",
    "import cv2\n",
    "\n",
    "def visualize_points_on_image(image, points):\n",
    "    for point in points:\n",
    "        x, y = int(point[0]), int(point[1])\n",
    "        if 0 <= x < image.shape[1] and 0 <= y < image.shape[0]:\n",
    "            cv2.circle(image, (x, y), 1, (0, 255, 0), -1)\n",
    "    return image\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    lidar_file = '0.bin'\n",
    "    calib_file = '0.txt'\n",
    "    image_file = '0.png'\n",
    "    output_image_file = '0_projected.png'\n",
    "    \n",
    "    points = load_velodyne_points(lidar_file)\n",
    "    calib = load_calibration(calib_file)\n",
    "    image = cv2.imread(image_file)\n",
    "    \n",
    "    projected_points = project_velo_to_cam2(points, calib)\n",
    "    \n",
    "    result_image = visualize_points_on_image(image, projected_points)\n",
    "    \n",
    "    cv2.imwrite(output_image_file, result_image)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 Generate inference result video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make inference result video\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "\n",
    "# set image path\n",
    "image_dir = '/home/eddie/depth_estimation/monodepth2/sit_data_inference/Three_way_Intersection/Three_way_Intersection_2/image_02/data'\n",
    "\n",
    "jpg_path = os.path.join(image_dir, 'jpg')\n",
    "jpeg_path = os.path.join(image_dir, 'jpeg')\n",
    "jpg_files = sorted(glob.glob(os.path.join(jpg_path, '*.jpg')))\n",
    "jpeg_files = sorted(glob.glob(os.path.join(jpeg_path, '*.jpeg')))\n",
    "\n",
    "temp_dir = os.path.join(image_dir, 'temp_images')\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "for i, (jpg_file, jpeg_file) in enumerate(zip(jpg_files, jpeg_files)):\n",
    "\n",
    "    img1 = cv2.imread(jpg_file)\n",
    "    img1 = cv2.resize(img1, (1920, 1200)) \n",
    "\n",
    "    img2 = cv2.imread(jpeg_file)\n",
    "    img2 = cv2.resize(img2, (1920, 1200)) \n",
    "\n",
    "    # concat images\n",
    "    combined_img = cv2.vconcat([img1, img2])\n",
    "\n",
    "    # save to tmp folder\n",
    "    temp_image_path = os.path.join(temp_dir, f\"{i:05d}.png\")\n",
    "    cv2.imwrite(temp_image_path, combined_img)\n",
    "\n",
    "output_video_path = os.path.join(image_dir, 'output_video_10.mp4')\n",
    "ffmpeg_cmd = [\n",
    "    'ffmpeg', '-y', '-framerate', '10', '-i', os.path.join(temp_dir, '%05d.png'),\n",
    "    '-c:v', 'libx264', '-pix_fmt', 'yuv420p', output_video_path\n",
    "]\n",
    "subprocess.run(ffmpeg_cmd)\n",
    "\n",
    "# remove temp folder\n",
    "# import shutil\n",
    "# shutil.rmtree(temp_dir)\n",
    "\n",
    "print(f\"Video saved to {output_video_path}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Generate inference result video (Multi-threading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make inference result video (Multi-threading)\n",
    "\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "import subprocess\n",
    "import concurrent.futures\n",
    "\n",
    "image_dir = '/home/eddie/depth_estimation/monodepth2/sit_data_inference/Cafeteria/Cafeteria_1/image_02/data'\n",
    "\n",
    "jpg_path = os.path.join(image_dir, 'jpg')\n",
    "jpeg_path = os.path.join(image_dir, 'jpeg')\n",
    "jpg_files = sorted(glob.glob(os.path.join(jpg_path, '*.jpg')))\n",
    "jpeg_files = sorted(glob.glob(os.path.join(jpeg_path, '*.jpeg')))\n",
    "\n",
    "# save to tmp folder\n",
    "temp_dir = os.path.join(image_dir, 'temp_images')\n",
    "os.makedirs(temp_dir, exist_ok=True)\n",
    "\n",
    "def process_images(i, jpg_file, jpeg_file):\n",
    "    img1 = cv2.imread(jpg_file)\n",
    "    img1 = cv2.resize(img1, (1920, 1200)) \n",
    "\n",
    "    img2 = cv2.imread(jpeg_file)\n",
    "    img2 = cv2.resize(img2, (1920, 1200)) \n",
    "\n",
    "    combined_img = cv2.vconcat([img1, img2])\n",
    "\n",
    "    temp_image_path = os.path.join(temp_dir, f\"{i:05d}.png\")\n",
    "    cv2.imwrite(temp_image_path, combined_img)\n",
    "\n",
    "# multi-threading\n",
    "with concurrent.futures.ThreadPoolExecutor() as executor:\n",
    "    futures = [\n",
    "        executor.submit(process_images, i, jpg_file, jpeg_file)\n",
    "        for i, (jpg_file, jpeg_file) in enumerate(zip(jpg_files, jpeg_files))\n",
    "    ]\n",
    "    for future in concurrent.futures.as_completed(futures):\n",
    "        future.result()\n",
    "\n",
    "output_video_path = os.path.join(image_dir, 'output_video_10.mp4')\n",
    "ffmpeg_cmd = [\n",
    "    'ffmpeg', '-y', '-framerate', '10', '-i', os.path.join(temp_dir, '%05d.png'),\n",
    "    '-c:v', 'libx264', '-pix_fmt', 'yuv420p', output_video_path\n",
    "]\n",
    "subprocess.run(ffmpeg_cmd)\n",
    "\n",
    "# remove tmp folder\n",
    "# import shutil\n",
    "# shutil.rmtree(temp_dir)\n",
    "\n",
    "print(f\"Video saved to {output_video_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
